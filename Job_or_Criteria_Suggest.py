import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import os
import streamlit as st

# Load Excel files
folder_path = "excel_files"  
all_data = []

# Read Excel files
for file_name in os.listdir(folder_path):
    if file_name.endswith(".xlsx"):
        file_path = os.path.join(folder_path, file_name)
        df = pd.read_excel(file_path)
        # Ensure required columns are present
        if "Ø³Ù…Øª" in df.columns and "Ø´Ø§Ø®Øµ" in df.columns:
            df = df[["Ø³Ù…Øª", "Ø´Ø§Ø®Øµ"]]
            all_data.append(df)
        else:
            print(f"Skipped {file_name}: Columns not found")

# Combine all data
if all_data:
    df = pd.concat(all_data, ignore_index=True)
else:
    st.error("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø² Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
    st.stop()

# Text preprocessing and vectorization
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df["Ø´Ø§Ø®Øµ"])

# Clustering
kmeans = KMeans(n_clusters=3, random_state=0)
df['Cluster'] = kmeans.fit_predict(X)

# Set up the UI
st.set_page_config(page_title="Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´ØºÙ„", layout="wide")
st.title("ğŸ¯ Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´ØºÙ„ ÛŒØ§ Ø´Ø§Ø®Øµ")
st.markdown("Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ±ÙˆØ¯ÛŒØŒ Ø´ØºÙ„ Ù…Ù†Ø§Ø³Ø¨ ÛŒØ§ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯.")

# Right-to-left alignment styling
st.markdown(
    """
    <style>
    body {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Choose the search type
search_type = st.radio("Ù†ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:", ["Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø§Ø®Øµ", "Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´ØºÙ„"])

if search_type == "Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø§Ø®Øµ":
    st.markdown("#### Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ú©Ø§Ù…Ø§ (,) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯:")
    user_input = st.text_input("Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§:", "")
    if user_input:
        # Convert input into a list
        input_list = [item.strip() for item in user_input.split(",")]

        # Compute similarities
        similarities = []
        for user_text in input_list:
            user_vector = vectorizer.transform([user_text])
            similarity = cosine_similarity(user_vector, X)
            similarities.append(similarity[0])

        # Calculate average similarity
        average_similarity = sum(similarities) / len(input_list)
        df['Average_Similarity'] = average_similarity

        # Sort and display suggested jobs
        suggested_jobs = df.sort_values(by="Average_Similarity", ascending=False)
        st.markdown("### âœ… Ø´ØºÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:")
        for index, row in suggested_jobs.drop_duplicates(subset=["Ø³Ù…Øª"]).head(10).iterrows():
            st.success(f"**{row['Ø³Ù…Øª']}** (Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª: {row['Average_Similarity']:.2f})")

elif search_type == "Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´ØºÙ„":
    st.markdown("#### Ø¹Ù†ÙˆØ§Ù† Ø´ØºÙ„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")
    job_input = st.text_input("Ø´ØºÙ„:", "")
    if job_input:
        # Filter data for the given job title
        filtered_data = df[df["Ø³Ù…Øª"].str.contains(job_input, case=False, na=False)]

        if not filtered_data.empty:
            st.markdown("### âœ… Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·:")
            for index, row in filtered_data.iterrows():
                st.info(f"**{row['Ø´Ø§Ø®Øµ']}**")
        else:
            st.warning(".Ø´ØºÙ„ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¹Ù†ÙˆØ§Ù† Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯")
